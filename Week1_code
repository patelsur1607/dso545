{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Installing-Required-Libraries\" data-toc-modified-id=\"Installing-Required-Libraries-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Installing Required Libraries</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-Familiar-With-Jupyter-Notebooks\" data-toc-modified-id=\"Getting-Familiar-With-Jupyter-Notebooks-0.0.1.1\"><span class=\"toc-item-num\">0.0.1.1&nbsp;&nbsp;</span>Getting Familiar With Jupyter Notebooks</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Week-1:-Basic-Python-Operations-for-Working-with-Text\" data-toc-modified-id=\"Week-1:-Basic-Python-Operations-for-Working-with-Text-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 1: Basic Python Operations for Working with Text</a></span></li><li><span><a href=\"#The-Scale-of-Data-in-the-21st-Century\" data-toc-modified-id=\"The-Scale-of-Data-in-the-21st-Century-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The Scale of Data in the 21st Century</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Text-Analytics\" data-toc-modified-id=\"Text-Analytics-2.1.0.1\"><span class=\"toc-item-num\">2.1.0.1&nbsp;&nbsp;</span>Text Analytics</a></span></li><li><span><a href=\"#Data-Engineering\" data-toc-modified-id=\"Data-Engineering-2.1.0.2\"><span class=\"toc-item-num\">2.1.0.2&nbsp;&nbsp;</span>Data Engineering</a></span></li><li><span><a href=\"#Statistics-/-Machine-Learning\" data-toc-modified-id=\"Statistics-/-Machine-Learning-2.1.0.3\"><span class=\"toc-item-num\">2.1.0.3&nbsp;&nbsp;</span>Statistics / Machine Learning</a></span></li></ul></li></ul></li><li><span><a href=\"#Loading-Text-into-Memory\" data-toc-modified-id=\"Loading-Text-into-Memory-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Loading Text into Memory</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Opening-Files\" data-toc-modified-id=\"Opening-Files-2.2.0.1\"><span class=\"toc-item-num\">2.2.0.1&nbsp;&nbsp;</span>Opening Files</a></span></li></ul></li><li><span><a href=\"#An-Aside:-List-Comprehension\" data-toc-modified-id=\"An-Aside:-List-Comprehension-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>An Aside: List Comprehension</a></span></li><li><span><a href=\"#Visualizing-Summary-Metrics-Using-Matplotlib\" data-toc-modified-id=\"Visualizing-Summary-Metrics-Using-Matplotlib-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Visualizing Summary Metrics Using Matplotlib</a></span></li><li><span><a href=\"#First-Method:-Create-a-Dictionary-to-Store-Word-Count\" data-toc-modified-id=\"First-Method:-Create-a-Dictionary-to-Store-Word-Count-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>First Method: Create a Dictionary to Store Word Count</a></span></li><li><span><a href=\"#Using-Python's-Built-In-Counter\" data-toc-modified-id=\"Using-Python's-Built-In-Counter-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Using Python's Built-In Counter</a></span></li><li><span><a href=\"#In-Class-Question\" data-toc-modified-id=\"In-Class-Question-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>In-Class Question</a></span></li></ul></li><li><span><a href=\"#Zipf's-Law\" data-toc-modified-id=\"Zipf's-Law-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Zipf's Law</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-Definition\" data-toc-modified-id=\"General-Definition-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>General Definition</a></span></li><li><span><a href=\"#Approximation-in-NLP\" data-toc-modified-id=\"Approximation-in-NLP-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Approximation in NLP</a></span></li></ul></li></ul></li><li><span><a href=\"#Regular-Expressions\" data-toc-modified-id=\"Regular-Expressions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regular Expressions</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Match-the-first-time-a-capital-letter-appears-in-the-tweet\" data-toc-modified-id=\"Match-the-first-time-a-capital-letter-appears-in-the-tweet-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Match the first time a capital letter appears in the tweet</a></span></li><li><span><a href=\"#Match-all-capital-letters-that-appears-in-the-tweet\" data-toc-modified-id=\"Match-all-capital-letters-that-appears-in-the-tweet-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Match all capital letters that appears in the tweet</a></span></li><li><span><a href=\"#Match-all-words-that-are-at-least-3-characters-long\" data-toc-modified-id=\"Match-all-words-that-are-at-least-3-characters-long-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Match all words that are at least 3 characters long</a></span></li><li><span><a href=\"#Word-Boundaries\" data-toc-modified-id=\"Word-Boundaries-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Word Boundaries</a></span></li><li><span><a href=\"#Removing-Stopwords-Using-Regex\" data-toc-modified-id=\"Removing-Stopwords-Using-Regex-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>Removing Stopwords Using Regex</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-3.0.5.1\"><span class=\"toc-item-num\">3.0.5.1&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Encoding-Schemes\" data-toc-modified-id=\"Encoding-Schemes-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Encoding Schemes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Implications-for-Data-Science\" data-toc-modified-id=\"Implications-for-Data-Science-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Implications for Data Science</a></span></li></ul></li><li><span><a href=\"#World-Languages,-in-Context\" data-toc-modified-id=\"World-Languages,-in-Context-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>World Languages, in Context</a></span></li><li><span><a href=\"#Bits-and-Bytes\" data-toc-modified-id=\"Bits-and-Bytes-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Bits and Bytes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li><li><span><a href=\"#ASCII\" data-toc-modified-id=\"ASCII-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>ASCII</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding/Decoding-Words\" data-toc-modified-id=\"Encoding/Decoding-Words-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Encoding/Decoding Words</a></span><ul class=\"toc-item\"><li><span><a href=\"#Steps:\" data-toc-modified-id=\"Steps:-4.3.1.1\"><span class=\"toc-item-num\">4.3.1.1&nbsp;&nbsp;</span>Steps:</a></span></li></ul></li><li><span><a href=\"#Python-Code\" data-toc-modified-id=\"Python-Code-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Python Code</a></span></li><li><span><a href=\"#Extended-ASCII\" data-toc-modified-id=\"Extended-ASCII-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Extended ASCII</a></span><ul class=\"toc-item\"><li><span><a href=\"#Latin-1\" data-toc-modified-id=\"Latin-1-4.3.3.1\"><span class=\"toc-item-num\">4.3.3.1&nbsp;&nbsp;</span>Latin-1</a></span></li><li><span><a href=\"#Excel-on-Macs\" data-toc-modified-id=\"Excel-on-Macs-4.3.3.2\"><span class=\"toc-item-num\">4.3.3.2&nbsp;&nbsp;</span>Excel on Macs</a></span></li></ul></li><li><span><a href=\"#Turning-Strings-into-Bytes\" data-toc-modified-id=\"Turning-Strings-into-Bytes-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>Turning Strings into Bytes</a></span></li><li><span><a href=\"#Turning-Bytes-into-Strings\" data-toc-modified-id=\"Turning-Bytes-into-Strings-4.3.5\"><span class=\"toc-item-num\">4.3.5&nbsp;&nbsp;</span>Turning Bytes into Strings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-4.3.5.1\"><span class=\"toc-item-num\">4.3.5.1&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li></ul></li><li><span><a href=\"#Unicode\" data-toc-modified-id=\"Unicode-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Unicode</a></span><ul class=\"toc-item\"><li><span><a href=\"#UTF-8\" data-toc-modified-id=\"UTF-8-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>UTF-8</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-Codepoint-of-a-Character\" data-toc-modified-id=\"Getting-the-Codepoint-of-a-Character-4.4.1.1\"><span class=\"toc-item-num\">4.4.1.1&nbsp;&nbsp;</span>Getting the Codepoint of a Character</a></span></li><li><span><a href=\"#Getting-the-Character-from-a-Codepoint\" data-toc-modified-id=\"Getting-the-Character-from-a-Codepoint-4.4.1.2\"><span class=\"toc-item-num\">4.4.1.2&nbsp;&nbsp;</span>Getting the Character from a Codepoint</a></span></li></ul></li><li><span><a href=\"#Variable-Length-Encoding-/Digitalization-and-Internationalization\" data-toc-modified-id=\"Variable-Length-Encoding-/Digitalization-and-Internationalization-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Variable Length Encoding /Digitalization and Internationalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Science-Implications\" data-toc-modified-id=\"Data-Science-Implications-4.4.2.1\"><span class=\"toc-item-num\">4.4.2.1&nbsp;&nbsp;</span>Data Science Implications</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Homework-1-(Due-Monday-March-23rd,-2020-at-11:59pm-PST)\" data-toc-modified-id=\"Homework-1-(Due-Monday-March-23rd,-2020-at-11:59pm-PST)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Homework 1 (Due Monday March 23rd, 2020 at 11:59pm PST)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Next-Week-(March-24th)\" data-toc-modified-id=\"Next-Week-(March-24th)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Next Week (March 24th)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-Understanding\" data-toc-modified-id=\"Check-for-Understanding-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Check for Understanding</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Familiar With Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter keyboard shortcuts:\n",
    "- Press `Esc` to go into **Command Mode**. Your cell should turn from green highlights to blue highlights.\n",
    "- In **Command Mode**, press `M` to go into `Markdown` mode. This turns your cell into Markdown text so you can type text.\n",
    "- Press `Y` to go into `Code` mode. This then allows you to begin typing Python code.\n",
    "- Press `A` to insert a cell above your current cell.\n",
    "- Press `B` to insert a cell below your current cell.\n",
    "- Press `D` twice to delete your current cell.\n",
    "- Press `Shift` + `Enter` to save your cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Basic Python Operations for Working with Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Scale of Data in the 21st Century\n",
    "<figure>\n",
    "  <img src=\"images/scale.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>ASCII table converting numbers to characters.<b>(Wikipedia)</b></i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "By the end of this week, you should be able to perform the following operations:\n",
    "\n",
    "\n",
    "#### Text Analytics\n",
    "- **load a text file into memory** using Python's built-in streaming libraries\n",
    "- **visualize word count and line length distributions** as histograms using Matplotlib\n",
    "\n",
    "#### Data Engineering\n",
    "- **read strings from a text input/output stream** using `readline()` and `readlines()`\n",
    "- **use both native Python dictionaries and `collections.Counter` objects** to produce word counts for a text corpus\n",
    "- perform basic search/replace operations using **regular expressions**\n",
    "- encode/decode text from bytes to support internationalization and digital-native characters (such as **emojis**).\n",
    "\n",
    "#### Statistics / Machine Learning\n",
    "- **create a word transition matrix using Numpy arrays**, which can be used for probabilistic inference and text generation (we will cover Week 2)\n",
    "\n",
    "\n",
    "## Loading Text into Memory\n",
    "\n",
    "There are a variety of ways to hold data within memory. For text analytics and natural language processing purposes, we'll be most concerned with the following:\n",
    "\n",
    "- **list**\n",
    "- **set**\n",
    "- **dictionary**\n",
    "- **tuple**\n",
    "- **Numpy array**\n",
    "\n",
    "Imagine that we would like to find the most commonly used words in ***A Tale of Two Cities***, by the famed English novelist Charles Dickens, stored in a text file called **`tale-of-two-cities.txt`**, in the same directory as this Jupyter notebook. Later on, we'll use 3rd-party libraries to automate much of the processing, but for now, we'll explore Python's built-in functions for text processing.\n",
    "\n",
    "#### Opening Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`open()`** function takes *two* parameters; **filename**, and **mode**. In our case, `mode` is set to `r` for **read**, since we plan to read the file's contents, as opposed to `w` (write), or `a` (append)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='tale-of-two-cities.txt' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "# Open Tale of Two Cities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a text character is **1 byte** in size. One byte is equal to **8 bits**. This means conceptually, the size of a string should be $N$ bytes, where $N$ is the number of characters. However, you'll see that in Python, the size of a string is larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of EMPTY_STRING is 49 bytes.\n",
      "The size of ONE_CHAR_STRING is 50 bytes.\n",
      "The size of TWO_CHAR_STRING is 51 bytes.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "EMPTY_STRING = \"\"\n",
    "ONE_CHAR_STRING = \"a\"\n",
    "TWO_CHAR_STRING = \"ab\"\n",
    "print(f\"The size of EMPTY_STRING is {sys.getsizeof(EMPTY_STRING)} bytes.\")\n",
    "print(f\"The size of ONE_CHAR_STRING is {sys.getsizeof(ONE_CHAR_STRING)} bytes.\")\n",
    "print(f\"The size of TWO_CHAR_STRING is {sys.getsizeof(TWO_CHAR_STRING)} bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`open()`** function returns a **`TextIOWrapper`** object from Python's `io` module, which handles common input/output streaming operations. \n",
    "\n",
    "A **stream** is a potentially infinite sequence of elements (in our case, characters) arriving over time. You'll use streams to model data that is **unbounded** (it's undetermined the volume, the length, and frequency of the data). A stream has a pointer to its current position within the sequence. \n",
    "\n",
    "This object has an extremely helpful **`readline()`** method that reads from a text file until encountering an **`EOF`** marker or a new line symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass in a parameter to **`readline()`** to control how many bytes of input stream data you'll receive. For instance, **`readline(2)`** returns at most 2 bytes of text input data. You might use this, for instance, if your Python application is reading not from a flat text file, but from a socket, which supplies a continuous stream of data with fixed length (ie., the messages all have the same number of characters).\n",
    "\n",
    "**In-Class Question**: *Assume you just opened the text file with **`open()`**. What output is returned when **`text_file.readline(5)`** is called the **second** time?*\n",
    "- **A)** The entire first line of the novel\n",
    "- **B)** The first 5 characters of the second line\n",
    "- **C)** The entire second line of the novel\n",
    "- **D)** The first 5 characters of the first line\n",
    "- **E)** The 6th-10th characters of the first line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:   IT \n",
      "Iteration 2: WAS t\n"
     ]
    }
   ],
   "source": [
    "text_file.seek(0) #reset the stream position to the start of the text file\n",
    "for i in range(2): # repeat the below line twice\n",
    "    print(f\"Iteration {i + 1}: {text_file.readline(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time that you call **`readline()`**, a position marker within **`TextIOWrapper`** is moved forward:\n",
    "<img src=\"images/readline.png\" width=\"500\" height=\"300\" align=\"center\"/>\n",
    "We typically will use **`readlines()`** instead to read text files line by line. This returns a Python **list**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # reset the stream position to the start of the file\n",
    " # read all the lines and return a list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are **12870** lines of text in the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of lines using F-strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the average number of characters per line\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # we are importing the pyplot module from matplotlib, and naming it as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Aside: List Comprehension\n",
    "\n",
    "Sometimes, we need to iterate through a list and perform some sort of operation (sum all the elements, or remove a certain character). The traditional way to do this is using a for loop:\n",
    "\n",
    "```Python\n",
    "lengths = [] # declare an empty list\n",
    "for line in lines: # iterate through each line\n",
    "    lengths.append(len(line)) # add the length of each line to the list\n",
    "```\n",
    "\n",
    "A slightly less verbose way, called **list comprehension**, to write this is\n",
    "\n",
    "```Python\n",
    "lengths = [len(line) for line in lines]\n",
    "```\n",
    "\n",
    "List comprehension is **typically slightly faster**, since it avoids the additional `append()` call for each iteration of the for loop. See this example from StackOverflow:\n",
    "\n",
    "```Python\n",
    "def slower(): # using traditional iteration\n",
    "    result = []\n",
    "    for elem in some_iterable:\n",
    "        result.append(elem)\n",
    "    return result\n",
    "```\n",
    "\n",
    "```Python\n",
    "def faster(): # using list comprehension\n",
    "    return [elem for elem in some_iterable]\n",
    "```\n",
    "\n",
    "Within the Python REPL **(read-eval-print-loop)**:\n",
    "\n",
    "```Python\n",
    ">>> some_iterable = range(1000)\n",
    ">>> import timeit\n",
    ">>> timeit.timeit('f()', 'from __main__ import slower as f', number=10000)\n",
    "1.4456570148468018\n",
    ">>> timeit.timeit('f()', 'from __main__ import faster as f', number=10000)\n",
    "0.49323201179504395\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Summary Metrics Using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = 30 # increase this number to make the visualization more granular\n",
    "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
    "plt.hist([len(line) for line in lines], bins=NUM_BINS)\n",
    "plt.title(\"Distribution of Line Lengths in Tale of Two Cities\") # give the plot a title\n",
    "plt.xlabel(\"Number of Characters in Line\") # label the X axis\n",
    "plt.ylabel(\"Count of Lines\") # label the Y axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if now we want to visualize how many times each word appears in the entire novel (for now, we won't worry about **stemming / lemmatization** and other preprocessing steps)?\n",
    "\n",
    "### First Method: Create a Dictionary to Store Word Count\n",
    "\n",
    "Dictionaries in Python have **keys** and **values**. The keys must be unique (no duplicate keys). They can be accessed via the **`keys()`** and **`values()`** methods of a dictionary object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Python **`sets`** to check that our dictionary's keys are unique. Remember that a set is a collection of **unique elements**, so calling **`set(words)`** will return only the unique words in our text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python's Built-In Counter\n",
    "\n",
    "Since the task of building a count using a dictionary is a common operation, Python provides a built-in object called `Counter` that we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core principle of software engineering and programming is **DRY**: Don't Repeat Yourself. Since we are likely going to be making many histograms throughout this course, it's best that we create a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_histogram(values, title=None,xlabel=None,ylabel=None, bins=30, x_size=15, y_size=6):\n",
    "    plt.rcParams[\"figure.figsize\"] = (x_size,y_size)\n",
    "    plt.hist(values, bins=bins)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title) # give the plot a title\n",
    "        \n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel) # label the X axis\n",
    "        \n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel) # label the Y axis\n",
    "        \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_histogram(word_count.values(), \n",
    "               title=\"Distribution of Word Count\",\n",
    "               xlabel=\"Number of Times Word Appears\",\n",
    "               ylabel=\"Number of Unique Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Question\n",
    "- Why does this distribution look the way it does? \n",
    "- What additional steps could be taken to make the results more meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # output the results to a dataframe\n",
    "# create a dataframe with two columns, word and frequency\n",
    "\n",
    "\n",
    "# save to an outputs folder - if you don't have one, Python will throw an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law\n",
    "\n",
    "### General Definition\n",
    "\n",
    "Zipf's Law states that for `N` words, the `k`th most frequent word will appear with a normalized frequency equal to\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/zipf.svg\" alt=\"my alt text\"/>\n",
    "</figure>\n",
    "\n",
    "The parameter $s$ is an exponent that defines the behavior of the distribution. Traditionally, in natural language, $s = 1$.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/zip_languages.png\" alt=\"my alt text\"/>\n",
    "  <figcaption><i>Stefan Evert, http://zipfr.r-forge.r-project.org/materials/LREC2018/tutorial_lrec2018.handout.pdf</i></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Approximation in NLP\n",
    "\n",
    "If $t_1$ is the most common word in a collection of text, and $t_2$ is the next most common word, then the frequency of the $i$th most common word is proportional to $\\frac{1}{i}$. The approximation we'll use specifically for natural languages is\n",
    "\n",
    "$$\n",
    "f(t_i) = \\frac{0.1}{i^\\alpha}\n",
    "$$\n",
    "\n",
    "$\\alpha = 1$.\n",
    "\n",
    "To represent the frequency of a word in a body of text.\n",
    "\n",
    "In human language, there are **a few high-frequency words and many low-frequency words**. What does this mean in terms of machine learning / data modelling?\n",
    "\n",
    "* In many cases, the high frequency words do not carry much value in terms of predictive power or signal. These are frequently **stopwords** that must be removed / otherwise feature-engineered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "SAMPLE_TWEET = '''\n",
    "#wolfram Alpha SUCKS! Even for researchers the information provided is less than you can get from \n",
    "#google or #wikipedia, totally useless! Avoid Wolfram at all costs, #ScrewWolframProducts\"\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match the first time a capital letter appears in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match all capital letters that appears in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match all words that are at least 3 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wolfram', 'Alpha', 'SUCKS', 'Even', 'for']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IT WAS the best of times, it was the worst o...</td>\n",
       "      <td>[WAS, the, best, times, was, the, worst, times...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>age of wisdom, it was the age of foolishness, ...</td>\n",
       "      <td>[age, wisdom, was, the, age, foolishness, was,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>belief, it was the epoch of incredulity, it wa...</td>\n",
       "      <td>[belief, was, the, epoch, incredulity, was, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was the season of Darkness, it was the spri...</td>\n",
       "      <td>[was, the, season, Darkness, was, the, spring,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>winter of despair, we had everything before us...</td>\n",
       "      <td>[winter, despair, had, everything, before, had...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                line  \\\n",
       "0    IT WAS the best of times, it was the worst o...   \n",
       "1  age of wisdom, it was the age of foolishness, ...   \n",
       "2  belief, it was the epoch of incredulity, it wa...   \n",
       "3  it was the season of Darkness, it was the spri...   \n",
       "4  winter of despair, we had everything before us...   \n",
       "\n",
       "                                             results  \n",
       "0  [WAS, the, best, times, was, the, worst, times...  \n",
       "1  [age, wisdom, was, the, age, foolishness, was,...  \n",
       "2  [belief, was, the, epoch, incredulity, was, th...  \n",
       "3  [was, the, season, Darkness, was, the, spring,...  \n",
       "4  [winter, despair, had, everything, before, had...  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Boundaries\n",
    "\n",
    "Consider the sentence:\n",
    "\n",
    "*A thorough examination of the movie shows Thor was a thorn in the side of the villains, both then and now. thor.*\n",
    "\n",
    "What happens if you try to parse out all `Thor` references? What happens if you want to remove `A` or `a`, or `the` to clean up the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A thorough examination of the movie shows Thor was a thorn in the side of the villains, both then and now. thor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thor', 'thor']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords Using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "1. One of the main characters in A Tale of Two Cities is `Sydney Carton`. How many times is the word `Carton` used?\n",
    "2. How many times does the word `the` appear in the novel?\n",
    "3. How would you find replace the stopword `the` using regex from `Tale of Two Cities`\n",
    "3. What percentage of lines in Dickens' text contain adverbs? For now, you can classify an adverb as a word that ends in `ly`.\n",
    "4. One many times does Charles Dickens use the pattern `WORD, WORD, and WORD` in this novel (for example `red, bluff, and free`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Schemes\n",
    "\n",
    "This is not a computer science class, but in your practical work, you will frequently have to deal with text that is encoded in a variety of styles. Understanding the difference between them is key.\n",
    "\n",
    "### Implications for Data Science\n",
    "\n",
    "Reasons I've encountered in my own work with NLP why it is beneficial to understand text encoding schemes:\n",
    "* If you are not using the right encoding, you cannot perform adequate feature engineering\n",
    "* Some data-scientists have simply \"thrown away\" samples of tweets, social media comments that seem \"mal-formed\" but are actually simply just using a different encoding scheme\n",
    "\n",
    "\n",
    "## World Languages, in Context\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/most_popular_languages.png\" alt=\"my alt text\"/>\n",
    "</figure>\n",
    "\n",
    "## Bits and Bytes\n",
    "\n",
    "- Computers, at its lowest level, store everything in the form of bits (either a 0 or a 1). The amount of information that can be represented in a computer is determined by the number of bits.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/hierarchy.png\" alt=\"my alt text\"/>\n",
    "</figure>\n",
    "\n",
    "For instance, a using only 4 bits, you can store **$2^4$ = 16** different values.\n",
    "<figure>\n",
    "  <img src=\"images/binary.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>How <b>$101010$</b> is converted to decimal (human-readable numbers): each of the green numbers is summed up to equal 42.</i></figcaption>\n",
    "</figure>\n",
    "\n",
    "It is not physically efficient for a computer to try to read one bit at a time, so typically data is stored in **8-bit** groups called **bytes**.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. How many bits (0s and 1s) does it take to represent **4 different characters**?\n",
    "2. How many bits (0s and 1s does it take to represent **128 different characters**?\n",
    "\n",
    "## ASCII\n",
    "\n",
    "The oldest, yet still relevant encoding style to be aware of is **ASCII**, where computers represent text (**every character on a keyboard**) initially as a number between 0 and 127 (question: how many bits does it take to do this?)\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/ascii.svg\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>ASCII table converting numbers to characters.<b>(Wikipedia)</b></i></figcaption>\n",
    "</figure>\n",
    "\n",
    "*If the smallest amount of data a computer can realistically read in is a byte (**8-bits**), why is ASCII only **7-bits**?* The last bit was a **parity bit** is used for **error checking** - to ensure that the data wasn't corrected or unintentionally altered.\n",
    "\n",
    "### Encoding/Decoding Words\n",
    "\n",
    "How would you write the word `Data` using ASCII encoding?\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Look up the \"codepoint\" for the first character (`D`).\n",
    "\n",
    "When you look up the character map value for `D`, its corresponding codepoint is `68`. Note that this is a different codepoint than lowercase `d`.\n",
    "\n",
    "2. Write out that number in binary.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/empty_binary_workbook.png\" alt=\"my alt text\"/>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/full_binary_workbook.png\" alt=\"my alt text\"/>\n",
    "</figure>\n",
    "\n",
    "The ASCII binary encoding for `D` is `1000100`.\n",
    "\n",
    "3. Repeat for `a`, `t`, and `a`.\n",
    "\n",
    "Use [this website to check your answer](https://www.rapidtables.com/convert/number/ascii-to-binary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find out your computer's default encoding system\n",
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Python Code\n",
    "\n",
    "Do not worry about understanding what is happening inside the `get_binary_for_char` and `get_binary` functions. Just know that they take in a string and produce the 0s and 1s that the string is encoded in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def get_binary_for_char(char: str, encoding=\"utf-8\") -> str:\n",
    "    \"\"\"\n",
    "    Encodes a character using the desired encoding into its corresponding hex, then converts the\n",
    "    hex code into binary, formatted with tab spaces between byte marks.\n",
    "    \"\"\"\n",
    "\n",
    "    hex_code = char.encode(encoding).hex()\n",
    "    code_point = hex(ord(char))[2:].upper()\n",
    "\n",
    "    binary: str = f\"{int(hex_code, 16):08b}\"\n",
    "\n",
    "    byte_list: List[str] = re.findall('[01]{8}', binary)\n",
    "    formatted_binary: str = \"\\t\".join(byte_list)  # for variable length encoding, tab space between byte marks.\n",
    "    print(f\"{char} (U+{code_point.zfill(4)}, hex:{hex_code}) - {encoding}: {formatted_binary}\")\n",
    "    return formatted_binary\n",
    "\n",
    "\n",
    "def get_binary(text: str, encoding=\"utf-8\"):\n",
    "    return \" \".join([get_binary_for_char(char, encoding) for char in text])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASCII encoding of \"Data\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended ASCII\n",
    "\n",
    "The dominant language in earlier eras of computing was English. People began to realize that ASCII was relatively limited, and even other European languages could not be properly supported. At the same time, transmission technology evolved to a standard of reliability such that the parity bit used for checking for errors was no longer needed. \n",
    "\n",
    "As a result, people began using the last (eighth) bit to extend the number of characters represented by ASCII from 128 characters to 256 characters.\n",
    "\n",
    "#### Latin-1\n",
    "\n",
    "Character map [available here](https://www.htmlhelp.com/reference/charset/latin1.gif).\n",
    "\n",
    "Characters such as `Ç` (pronounced `ch` in Turkish, for instance), is represented by the number `199`. The Spanish word `año` (year) includes a character `ñ` that would be represented by the code point `241`.\n",
    "\n",
    "#### Excel on Macs\n",
    "\n",
    "Macs commonly use [Mac OS Roman encoding](https://en.wikipedia.org/wiki/Mac_OS_Roman). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latin1 encoding of \"cat\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding Cuántas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, to convert from bytes to strings, you use the `encode()` and `decode()` functions. \n",
    "\n",
    "### Turning Strings into Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latin1 encoding: b'm\\xe1s'\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character '\\xe1' in position 1: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f27879f083c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using Python's encode/decode:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using latin1 encoding:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"más\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using ASCII encoding:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"más\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character '\\xe1' in position 1: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# Using Python's encode/decode:\n",
    "print(\"Using latin1 encoding:\", \"más\".encode(encoding=\"latin1\"))\n",
    "print(\"Using ASCII encoding:\", \"más\".encode(encoding=\"ascii\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Bytes into Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "file = open(\"test.txt\", mode=\"rb\")\n",
    "bytes_from_file = file.read()\n",
    "print(type(bytes_from_file))\n",
    "string_from_file = bytes_from_file.decode()\n",
    "print(type(string_from_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "1. Encode the word `más` in ASCII.\n",
    "2. Encode the word `más` in `latin1`.\n",
    "3. Decode the binary stream `01100011 01100001 01110100`. Assume that it is using `latin1` encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode\n",
    "\n",
    "128 characters is not enough to represent the characters in other Languages, like **Greek, Turkish, Cyrillic**, etc., or newer social media phenomenons like **emojis**. Unicode stores text as either 8, 16, or 32 bits (1, 2, or 4 bytes). This means there's significantly more characters that can be encoded (approximately 1 billion characters).\n",
    "\n",
    "As a point of reference, there's a total of **50,000** characters in the Chinese language (but only around **15-20,000** that are used commonly).\n",
    "\n",
    "If you don't specify the right encoding to read in text, you'll end up with something like this:\n",
    "<figure>\n",
    "  <img src=\"images/mojibake.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>Malformed characters because of incorrect encoding.<b>(Wikipedia)</b></i></figcaption>\n",
    "</figure>\n",
    "\n",
    "### UTF-8\n",
    "\n",
    "The default encoding scheme of the internet today is `UTF-8`.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/encoding_shares.svg\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>Share of web pages with different encodings.<b>(Wikipedia)</b></i></figcaption>\n",
    "</figure>\n",
    "\n",
    "There is another encoding schema very similar to `UTF-8` called `UTF-16`. You'll typically find it being used on Windows systems and within Java applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the Codepoint of a Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the Character from a Codepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'am'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97) + chr(109)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Variable Length Encoding /Digitalization and Internationalization\n",
    "\n",
    "UTF-8 is the default encoding schema of the internet. Whenever you save files to disk, or read files in, your first choice should be to try using UTF-8. UTF-8 is an example of **variable-length encoding**. This means sometimes a character will take 8 bits to encode (represent), sometimes 16 bits, sometimes 24 bits, and sometimes 32 bits.\n",
    "\n",
    "UTF-8 uses **continuation bytes** for any character it needs to represent beyond 1 byte. These bytes always start with `10`.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/continuation.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>UTF-8 Continuation Bytes</i></figcaption>\n",
    "</figure>\n",
    "\n",
    "On the other hand, another encoding scheme is `UTF-32`- it always takes **32 bits**. \n",
    "\n",
    "#### Data Science Implications\n",
    "UTF-8 should be your default encoding of choice when working with Big Data. Because the # of bits it takes to encode a character changes, it can be more \"storage-efficient\" on disk, and more \"memory-efficient\" when representing this text in memory. \n",
    "\n",
    "Many machine-learning algorithms (like **batch and mini-batch gradient descent**) will perform updates using batches of samples  (if not the entire dataaset). If you choose the wrong encoding, you will not be able to fit as many samples into your batch for training as you'd like - this means your model may require significantly more training time and perform worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_binary(\"I 😍 DSO 599\", encoding=\"ascii\")\n",
    "# get_binary(\"I 😍 DSO 599\", encoding=\"latin1\")\n",
    "# get_binary(\"I 😍 DSO 599\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 (Due Monday March 23rd, 2020 at 11:59pm PST)\n",
    "\n",
    "Every day late is -10%.\n",
    "\n",
    "You are a business analyst working for a major US toy retailer:\n",
    "\n",
    "* A manager in the marketing department wants to find out the most frequently used words in positive reviews (five stars) and negative reviews (one star) in order to determine what occasion the toys are purchased for (Christmas, birthdays, and anniversaries.). He would like your opinion on **which gift occasions (Christmas, birthdays, or anniversaries) tend to have the most positive reviews** to focus marketing budget on those days.\n",
    "\n",
    "* One of your product managers suspects that **toys purchased for male recipients (husbands, sons, etc.)** tend to be much more likely to be reviewed poorly. She would like to see some data points confirming or rejecting her hypothesis. \n",
    "\n",
    "* Use **regular expressions to parse out all references to recipients and gift occassions**, and account for the possibility that people may spell words \"son\" / \"children\" / \"Christmas\" as both singular and plural, upper or lower-cased.\n",
    "\n",
    "* Explain what some of pitfalls/limitations are of using only a word count analysis to make these inferences. What additional research/steps would you need to do to verify your conclusions?\n",
    "\n",
    "* Create a simple Excel CSV file that contains 2-3 lines at most describing yourself, your background, and interests. It must contain at least 1 emoji and 4-5 international characters (non-ASCII). Make sure to properly encode the file so that I can open it in `UTF-8` to read. Attach it to your submission.\n",
    "\n",
    "Perform the same word count analysis using the reviews received from Amazon to answer your marketing manager's question. They are stored in two files, (`poor_amazon_toy_reviews.txt`) and (`good-amazon-toy-reviews.txt`). **Provide a few sentences with your findings and business recommendations.** Make any assumptions you'd like to- this is a fictitious company after all. I just want you to get into the habit of \"finishing\" your analysis: to avoid delivering technical numbers to a non-technical manager.\n",
    "\n",
    "**Submit everything as a new notebook and Slack direct message to me (Yu Chen) the HW as an attachment.**\n",
    "\n",
    "`NOTE`: Name the notebook `lastname_firstname_HW1.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Week (March 24th)\n",
    "\n",
    "* `scikit-learn`, `nltk`, and `scipy` libraries for NLP (make sure to install each of these libraries\n",
    "* Bayes Rule, Naive Bayes, probability theory for text classification\n",
    "* Similiarity/distance measures\n",
    "* N-Grams\n",
    "* Tokenization, lemmatization, stemming\n",
    "* Basic word vectors: Count, TF-IDF, One-Hot encoding\n",
    "* Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding\n",
    "\n",
    "1. Which of the encodings below will be able to encode this text: `사업`\n",
    "2. **True or False**: the word `dog` will have the same binary representation regardless of whether it is `ASCII`, `latin1`, or `utf8`.\n",
    "3. According to the Zipf Law approximation, approximately what frequency (express it has a percent) would the 3rd most popular word in a generic piece of text appear?\n",
    "4. **True or False**: what is considered a stopword changes depending on the business context and dataset you are working with. If true, provide an example. If false, explain why it is false."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
